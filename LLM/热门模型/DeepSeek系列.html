<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="keywords" content="开源工具,学习笔记" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #252232;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode = window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://jasonaowu.github.io/jasonNote/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/DeepSeek%E7%B3%BB%E5%88%97.html"><meta property="og:site_name" content="JasonCC Blog"><meta property="og:title" content="DeepSeek系列"><meta property="og:description" content="问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节..."><meta property="og:type" content="website"><meta property="og:image" content="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/cb648375-82d9-4f82-9fbe-d2215310d62c.webp"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-04-26T09:54:14.000Z"><meta property="article:modified_time" content="2025-04-26T09:54:14.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebPage","name":"DeepSeek系列","description":"问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节..."}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"><link rel="alternate" type="application/atom+xml" href="https://jasonaowu.github.io/jasonNote/jasonNote/atom.xml" title="JasonCC Blog Atom Feed"><link rel="alternate" type="application/json" href="https://jasonaowu.github.io/jasonNote/jasonNote/feed.json" title="JasonCC Blog JSON Feed"><link rel="alternate" type="application/rss+xml" href="https://jasonaowu.github.io/jasonNote/jasonNote/rss.xml" title="JasonCC Blog RSS Feed"><link rel="icon" href="/jasonNote/favicon.ico"><title>DeepSeek系列 | JasonCC Blog</title><meta name="description" content="问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节...">
    <link rel="stylesheet" href="/jasonNote/assets/css/styles.cb9aac79.css">
    <link rel="preload" href="/jasonNote/assets/js/runtime~app.142c6249.js" as="script"><link rel="preload" href="/jasonNote/assets/css/styles.cb9aac79.css" as="style"><link rel="preload" href="/jasonNote/assets/js/1316.1b8cebb6.js" as="script"><link rel="preload" href="/jasonNote/assets/js/app.3756072b.js" as="script">
    
    <!-- 统计代码区域-->
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/jasonNote/" aria-label="带我回家"><img class="vp-nav-logo" src="/jasonNote/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">JasonCC Blog</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/jasonNote/blog.html" aria-label="博客" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:blog" height="1em" sizing="height"></iconify-icon><!--]-->博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="应用"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bars-staggered" height="1em" sizing="height"></iconify-icon>应用<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/apps/Chrome.html" aria-label="常用扩展" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-brands:chrome" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->常用扩展<!----></a></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Intership</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/jasonNote/apps/topic/" aria-label="搜广推" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:dice-d20" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->搜广推<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="生活"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bed-pulse" height="1em" sizing="height"></iconify-icon>生活<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/family/Diet.html" aria-label="健康饮食" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:wheat-awn-circle-exclamation" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->健康饮食<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/family/Coupon.html" aria-label="网购攻略" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:cart-shopping" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->网购攻略<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="工具"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:toolbox" height="1em" sizing="height"></iconify-icon>工具<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://www.aishort.top/" aria-label="ChatGPT SC" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bolt" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->ChatGPT SC<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://prompt.newzone.top/" aria-label="IMGPrompt" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:image" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->IMGPrompt<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://tools.newzone.top/json-translate" aria-label="多语言翻译" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:language" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->多语言翻译<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://nav.newzone.top/" aria-label="工具收藏" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bars" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->工具收藏<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://discord.gg/PZTQfJ4GjX" target="_blank" rel="noopener noreferrer" aria-label="discord"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 640" style="width:1.25rem;height:1.25rem;vertical-align:middle"><path d='m534.42,126.58a1.57,1.57 0 0 0 -0.79,-0.73a507.33,507.33 0 0 0 -125.19,-38.81a1.9,1.9 0 0 0 -2.01,0.95a352.96,352.96 0 0 0 -15.58,32.01a468.41,468.41 0 0 0 -140.6,0a323.75,323.75 0 0 0 -15.82,-32.01a1.98,1.98 0 0 0 -2.02,-0.95a505.9,505.9 0 0 0 -125.19,38.81a1.79,1.79 0 0 0 -0.82,0.7c-79.73,119.07 -101.57,235.21 -90.86,349.9a2.11,2.11 0 0 0 0.81,1.44a510.06,510.06 0 0 0 153.56,77.6a1.99,1.99 0 0 0 2.15,-0.71a364.19,364.19 0 0 0 31.42,-51.08a1.95,1.95 0 0 0 -1.07,-2.71a335.92,335.92 0 0 1 -47.98,-22.85a1.98,1.98 0 0 1 -0.19,-3.27c3.22,-2.42 6.44,-4.93 9.53,-7.46a1.9,1.9 0 0 1 1.99,-0.27c100.65,45.94 209.61,45.94 309.07,0a1.89,1.89 0 0 1 2.01,0.24c3.09,2.54 6.31,5.07 9.55,7.49a1.97,1.97 0 0 1 -0.17,3.27a315.25,315.25 0 0 1 -48,22.83a1.97,1.97 0 0 0 -1.05,2.73a409.02,409.02 0 0 0 31.4,51.05a1.95,1.95 0 0 0 2.15,0.73a508.37,508.37 0 0 0 153.81,-77.59a1.97,1.97 0 0 0 0.81,-1.41c12.82,-132.61 -21.48,-247.79 -90.93,-349.9zm-315.91,280.03c-30.3,0 -55.27,-27.81 -55.27,-61.96s24.48,-61.96 55.27,-61.96c31.03,0 55.76,28.05 55.27,61.96c0,34.15 -24.48,61.96 -55.27,61.96zm204.35,0c-30.3,0 -55.27,-27.81 -55.27,-61.96s24.48,-61.96 55.27,-61.96c31.03,0 55.76,28.05 55.27,61.96c0,34.15 -24.23,61.96 -55.27,61.96z' fill="currentColor"/></svg></a></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/jasonaowu/jasonNote" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/BERT.html" aria-label="BERT" iconsizing="both"><!---->BERT<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/DeepSeek%E7%B3%BB%E5%88%97.html" aria-label="DeepSeek系列" iconsizing="both"><!---->DeepSeek系列<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/GPT.html" aria-label="GPT系列" iconsizing="both"><!---->GPT系列<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/Qwen.html" aria-label="Qwen" iconsizing="both"><!---->Qwen<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->DeepSeek系列</h1><div class="page-info"><!----><!----><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon" name="word"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 2107 字</span><meta property="wordCount" content="2107"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 7 分钟</span><meta property="timeRequired" content="PT7M"></span><!----></div><hr></div><!----><!----><div class="theme-hope-content" vp-content><h2 id="问题和结论" tabindex="-1"><a class="header-anchor" href="#问题和结论"><span>问题和结论</span></a></h2><ol><li>MOE 能否做知识蒸馏？ <span style="color:blue;"><strong>可以</strong></span></li><li>MOE 相比 Dense 的优势？ <span style="color:blue;"><strong>节约计算成本</strong></span></li><li>什么结构能用 MOE？ <span style="color:blue;"><strong>任何 FFN → 多任务问题</strong></span></li><li>在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ <span style="color:blue;"><strong>便宜</strong></span></li><li>个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节约计算量，也可以节约存储量，是否正确？ <span style="color:blue;"><strong>正确</strong></span></li></ol><blockquote><p><strong>💡错误观点</strong>：</p><ol><li>MOE 是为了减小网络结构？ ❌ 相反，MOE的初衷是为了在保证较低计算量的同时，增加模型参数，使模型更强</li><li>MOE 是为了将深层网络变为浅层网络？ ❌ 将中间层参数数量从 N，降低为 N/E，分散到E个专家上，可能可以将网络变浅，但这不是主要目的 原来以为MOE是针对深层网络做的优化，将深层网络变为浅层网络，但是实际是</li></ol></blockquote><h3 id="deepseek-研究脉络" tabindex="-1"><a class="header-anchor" href="#deepseek-研究脉络"><span>DeepSeek 研究脉络</span></a></h3><p>💡 <strong>从稠密模型到混合专家，再到推理方向</strong></p><p>回顾 DeepSeek 过去一年多发表的核心论文，我们大致能将其研究分为两条主要脉络：</p><ul><li><strong>基座模型（Foundation Models）</strong>：从最早的 Dense（稠密）结构一路演进到 MOE（混合专家）模式，并在这个过程中不断发明和采用新的高效训练算法。</li><li><strong>推理能力（Reasoning）</strong>：包括解数学题、代码生成、逻辑问答乃至定理证明等，更强调大模型的&quot;思考深度&quot;，并在如何进行强化学习方面进行了连续多次创新。</li></ul><h2 id="moe-基本原理" tabindex="-1"><a class="header-anchor" href="#moe-基本原理"><span>MOE 基本原理</span></a></h2><p>MOE 全称是 Mixture of Experts，也就是混合专家模型</p><h3 id="最最最原始版" tabindex="-1"><a class="header-anchor" href="#最最最原始版"><span>最最最原始版</span></a></h3><h4 id="组成" tabindex="-1"><a class="header-anchor" href="#组成"><span>组成</span></a></h4><ol><li><strong>稀疏 MOE 层</strong>：n 个专家 FFN</li><li><strong>路由</strong>：token 到 top-K 个专家</li></ol><h4 id="ffn-对比" tabindex="-1"><a class="header-anchor" href="#ffn-对比"><span>FFN 对比</span></a></h4><ul><li><strong>Vs Transformer</strong></li></ul><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/cb648375-82d9-4f82-9fbe-d2215310d62c.webp" alt="Transformer对比" tabindex="0" loading="lazy"><figcaption>Transformer对比</figcaption></figure><table><thead><tr><th style="text-align:center;"><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/f61ccc9d-e249-4399-b5d5-fe4047576725.webp" alt="FFn" loading="lazy"></th><th style="text-align:center;"><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/c7c02a38-3840-4f5e-ad60-06dc6ece64b2.webp" alt="MOE公式" loading="lazy"></th></tr></thead><tbody><tr><td style="text-align:center;">FFN</td><td style="text-align:center;">MOE</td></tr></tbody></table><ul><li><strong>一般的 gating network 的计算，便于和 deepseek 做对比</strong></li></ul><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/0095a6a0-a489-42cc-86d5-6674fa92d8df.webp" alt="0095a6a0-a489-42cc-86d5-6674fa92d8df" tabindex="0" loading="lazy"><figcaption>0095a6a0-a489-42cc-86d5-6674fa92d8df</figcaption></figure><h4 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势</span></a></h4><ul><li>相比 dense 模型，<strong>预训练速度更快</strong></li><li>相比同参数量模型，<strong>推理速度更快</strong></li><li>但是需要高 VRAM，因为所有专家都加载在内存中</li></ul><blockquote><p>一个最直观的数据：</p><p>在 DeepSeek 官网上看到，DeepSeek-V3、V2.5 版本都用了 MoE 架构。但像 Qwen、LLama 模型，用的却是 Dense 架构，也就是传统的 Transformer 架构。这两种架构有个很明显的区别。DeepSeek-V3 版本总参数量高达 6710 亿，可每次计算激活的参数量，也就是真正参与到计算里的参数，只有 370 亿，是总参数量的 5.5%。但 Qwen 和 LLama 模型就不一样了，它们每次计算激活的参数量，就是整个模型的参数量，没有 “打折”。</p></blockquote><h3 id="switch-transformer" tabindex="-1"><a class="header-anchor" href="#switch-transformer"><span>Switch Transformer</span></a></h3><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/18fc4416-2296-4eb5-910e-a0b5b0984782.webp" alt="18fc4416-2296-4eb5-910e-a0b5b0984782" tabindex="0" loading="lazy"><figcaption>18fc4416-2296-4eb5-910e-a0b5b0984782</figcaption></figure><h2 id="deepseek-moe-2024-01" tabindex="-1"><a class="header-anchor" href="#deepseek-moe-2024-01"><span>DeepSeek MOE(2024.01)</span></a></h2><p>DeepSeek-V1 应该是 2023 年 12 月的 DeepSeek LLM Base 和 Chat 模型，是稠密模型。</p><p>DeepSeek-V2 及其之后的模型用的都是 MoE 了。</p><p><a href="https://arxiv.org/pdf/2401.06066" target="_blank" rel="noopener noreferrer">DeepSeek MOE 原文</a></p><h3 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h3><ul><li><p>LLM 中，<span style="color:blue;"><strong>扩展模型参数时节约成本</strong></span>，故使用 MoE</p></li><li><p>Deepseek MOE 就是为了通过更加高效的机制来确保专家之间的任务分配具有更高的<span style="color:blue;"><strong>专门化</strong></span>。</p></li><li><p>无法确保专家的专门化：这种重叠会导致专家没有获得足够的独特知识，也使得专家之间的差异化不明显，限制了模型的性能和效率。</p><ul><li><strong>知识混杂性（Knowledge Hybridity）</strong>：在传统的 MoE 架构中，通常只使用有限数量的专家（例如 8 个或 16 个）。当某个 token 被分配给某个专家时，这些专家所涵盖的知识往往是多样化的，因此该专家的参数会试图同时存储和处理非常不同类型的知识。这种知识的多样性和复杂性导致专家的知识无法高度专注和聚焦，从而难以在同一模型中有效地利用这些不同类型的知识。</li><li><strong>知识冗余性（Knowledge Redundancy）</strong>：在 MoE 架构中，不同的专家可能需要共享相同的知识。当多个专家被分配到类似的任务时，它们可能会重复学习和存储相同的知识，这导致了多个专家之间的知识冗余，浪费了存储资源，同时也限制了专家在其各自领域的专门化，使其无法达到 MoE 模型的理论上限性能。</li></ul></li></ul><h3 id="基本思想" tabindex="-1"><a class="header-anchor" href="#基本思想"><span>基本思想</span></a></h3><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/37d134bb-b70c-40db-bc79-7e9857fb9364.webp" alt="37d134bb-b70c-40db-bc79-7e9857fb9364" tabindex="0" loading="lazy"><figcaption>37d134bb-b70c-40db-bc79-7e9857fb9364</figcaption></figure><h4 id="精细化专家划分" tabindex="-1"><a class="header-anchor" href="#精细化专家划分"><span><strong>精细化专家划分</strong></span></a></h4><blockquote><p>划分更细，专家更加专业化，同时可以路由到更多的专家</p></blockquote><p>在保持参数总量不变的情况下，我们通过拆分 FFN 的中间隐藏层维度来对专家进行更加精细的划分。同时，我们激活更多的精细化专家，从而实现更灵活、更适应的专家组合。精细化的专家划分允许多样化的知识更加细致地分解和学习，从而使每个专家能够专注于更高层次的专业化任务。专家激活的灵活性增加，也有助于更准确和针对性地获取知识。</p><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/1b164aa0-fc5a-4b89-8cff-69294513d65e.webp" alt="1b164aa0-fc5a-4b89-8cff-69294513d65e" tabindex="0" loading="lazy"><figcaption>1b164aa0-fc5a-4b89-8cff-69294513d65e</figcaption></figure><h4 id="共享专家隔离" tabindex="-1"><a class="header-anchor" href="#共享专家隔离"><span><strong>共享专家隔离</strong></span></a></h4><p>我们将部分专家隔离出来，作为“共享专家”，始终被激活，用于捕捉和整合不同上下文中的共享知识。通过将共享知识压缩到这些共享专家中，减少了其他专家之间的冗余，从而提高了参数的效率，确保每个路由专家能够专注于独特的领域，保持高水平的专门化。</p><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/3aeff8bc-1d9d-4e03-a101-fd6d34c6b31e.webp" alt="3aeff8bc-1d9d-4e03-a101-fd6d34c6b31e" tabindex="0" loading="lazy"><figcaption>3aeff8bc-1d9d-4e03-a101-fd6d34c6b31e</figcaption></figure><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/2d03bf2c-1142-416b-8921-739967b392e9.webp" alt="2d03bf2c-1142-416b-8921-739967b392e9" tabindex="0" loading="lazy"><figcaption>2d03bf2c-1142-416b-8921-739967b392e9</figcaption></figure><h4 id="负载均衡问题" tabindex="-1"><a class="header-anchor" href="#负载均衡问题"><span><strong>负载均衡问题</strong></span></a></h4><blockquote><p>MOE 类模型的通病</p><p>虽然稀疏门控能在不增加计算成本的情况下显著扩展模型参数空间，但其性能高度依赖门控机制的有效性。门控机制无法控制发给专家的 token 的概率，所以在实际操作中，会存在专家间工作负载分布不均衡的情况。某些专家被频繁使用（接收到了很多 token）而其他专家却很少被调用（接收的 token 寥寥无几）。这不仅不符合 MoE 的设计初衷（术业有专攻），还影响计算效率（例如引起分布式训练中各卡通讯时的负载不均）。</p></blockquote><p>负载不均衡会造成：</p><ol><li>模型始终选择少数几个专家，其他专家缺乏充分训练，甚至部分专家参数完全没有更新</li><li>专家并行计算时计算瓶颈（分到 16 张卡上，花了 16 张卡的运行时的钱，只有一张卡在工作）</li></ol><p>解决方案：</p><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/ef67d1fe-4a22-4d68-ba99-6c6b94062200.webp" alt="ef67d1fe-4a22-4d68-ba99-6c6b94062200" tabindex="0" loading="lazy"><figcaption>ef67d1fe-4a22-4d68-ba99-6c6b94062200</figcaption></figure><h5 id="专家级负载均衡" tabindex="-1"><a class="header-anchor" href="#专家级负载均衡"><span>专家级负载均衡</span></a></h5><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/893fd346-4dfe-4c4a-9fe9-562173ef022f.webp" alt="893fd346-4dfe-4c4a-9fe9-562173ef022f" tabindex="0" loading="lazy"><figcaption>893fd346-4dfe-4c4a-9fe9-562173ef022f</figcaption></figure><figure><img src="https://blog-1316756713.cos.ap-shanghai.myqcloud.com/bolg/24555d94-2d6d-4920-b1df-47016d853dd3.webp" alt="24555d94-2d6d-4920-b1df-47016d853dd3" tabindex="0" loading="lazy"><figcaption>24555d94-2d6d-4920-b1df-47016d853dd3</figcaption></figure><h5 id="设备级负载均衡" tabindex="-1"><a class="header-anchor" href="#设备级负载均衡"><span>设备级负载均衡</span></a></h5><h2 id="deepseek-v2" tabindex="-1"><a class="header-anchor" href="#deepseek-v2"><span>DeepSeek-V2</span></a></h2><p>进一步优化负载均衡</p><h2 id="deepseek-v3-reasoning-model" tabindex="-1"><a class="header-anchor" href="#deepseek-v3-reasoning-model"><span>DeepSeek-V3(Reasoning model)</span></a></h2><ol><li>门控函数优化</li></ol><blockquote><p>首先 V3 的模型远大于 V2，V3 的每层 MOE 中有 256 个路由专家，8 个激活专家。但 V2 中只有 160 个路由专家，6 个激活专家，从参数上就可以发现 V3 的门控函数计算量远大于 V2，大家也都清楚当计算维度变大时 SoftMax 的前向和反向是很耗费计算资源的，而 Sigmod 直接将数值映射到[0，1]之间，相对来说更加简单。可能实现效果也类似，因此为了更加高效的训练从而进行了替换。</p></blockquote><ol><li>进一步优化负载均衡 <ol><li>无辅助损失的负载均衡</li><li>互补序列层面的辅助损失</li></ol></li></ol><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ul><li><a href="https://kevincheung2259.github.io/2024/09/13/MOE-Intro/index.html" target="_blank" rel="noopener noreferrer">MOE 介绍</a></li><li><a href="https://deepseek.csdn.net/67fa2941da5d787fd5cb6acb.html" target="_blank" rel="noopener noreferrer">DeepSeek 技术解析</a></li><li><a href="https://www.cnblogs.com/rossiXYZ/p/18835426#0x00-%E6%A6%82%E8%BF%B0" target="_blank" rel="noopener noreferrer">负载均衡部分参考资料</a></li></ul></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/4/26 09:54:14</span></div><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/BERT.html" aria-label="BERT" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->BERT</div></a><a class="route-link auto-link next" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/GPT.html" aria-label="GPT系列" iconsizing="both"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">GPT系列<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div style="display: flex;
align-items: center;
justify-content: center;
height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);
--icon-size: 48px;
display: inline-block;
width: var(--icon-size);
height: var(--icon-size);
background-color: currentcolor;
-webkit-mask-image: var(--loading-icon);
mask-image: var(--loading-icon);
"></span></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><!----><div class="vp-copyright">
  版权声明：自由转载 - 非商用 - 非衍生 - 保持署名<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" target="_blank" rel="noopener noreferrer">（创意共享 4.0 许可证）</a>|
  Copyright © 2023-present LearnData</a>
  </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/jasonNote/assets/js/runtime~app.142c6249.js" defer></script><script src="/jasonNote/assets/js/1316.1b8cebb6.js" defer></script><script src="/jasonNote/assets/js/app.3756072b.js" defer></script>
    <!-- 看板娘区块 -->
    <script src="/jasonNote/live2d-widget/autoload.js"></script>
    <!-- End 看板娘区块 -->
  </body>
</html>
