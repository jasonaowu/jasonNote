<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="keywords" content="开源工具,学习笔记" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #252232;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode = window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://jasonaowu.github.io/jasonNote/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/DeepSeek%E7%B3%BB%E5%88%97.html"><meta property="og:site_name" content="JasonCC Blog"><meta property="og:title" content="DeepSeek系列"><meta property="og:description" content="问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节..."><meta property="og:type" content="website"><meta property="og:image" content="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=NWVjNjk1ZmU5YmUyNDA3ZGZlZGQ3MzljZWU5NzVhZWJfQ1pESDhYRFRIM1QwNVEwTzRJSVNCdWZOS1FkNHJ5Q2JfVG9rZW46UnA1M2Jja25Cb2c2MlJ4TTc5SGNtMFE1bmdlXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-04-25T14:51:15.000Z"><meta property="article:modified_time" content="2025-04-25T14:51:15.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebPage","name":"DeepSeek系列","description":"问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节..."}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css"><link rel="alternate" type="application/atom+xml" href="https://jasonaowu.github.io/jasonNote/jasonNote/atom.xml" title="JasonCC Blog Atom Feed"><link rel="alternate" type="application/json" href="https://jasonaowu.github.io/jasonNote/jasonNote/feed.json" title="JasonCC Blog JSON Feed"><link rel="alternate" type="application/rss+xml" href="https://jasonaowu.github.io/jasonNote/jasonNote/rss.xml" title="JasonCC Blog RSS Feed"><link rel="icon" href="/jasonNote/favicon.ico"><title>DeepSeek系列 | JasonCC Blog</title><meta name="description" content="问题和结论 MOE 能否做知识蒸馏？ 可以 MOE 相比 Dense 的优势？ 节约计算成本 什么结构能用 MOE？ 任何 FFN → 多任务问题 在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ 便宜 个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节...">
    <link rel="stylesheet" href="/jasonNote/assets/css/styles.cb9aac79.css">
    <link rel="preload" href="/jasonNote/assets/js/runtime~app.7e62fd9b.js" as="script"><link rel="preload" href="/jasonNote/assets/css/styles.cb9aac79.css" as="style"><link rel="preload" href="/jasonNote/assets/js/1316.1b8cebb6.js" as="script"><link rel="preload" href="/jasonNote/assets/js/app.a956ffe5.js" as="script">
    
    <!-- 统计代码区域-->
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/jasonNote/" aria-label="带我回家"><img class="vp-nav-logo" src="/jasonNote/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">JasonCC Blog</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/jasonNote/blog.html" aria-label="博客" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:blog" height="1em" sizing="height"></iconify-icon><!--]-->博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="应用"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bars-staggered" height="1em" sizing="height"></iconify-icon>应用<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/apps/Chrome.html" aria-label="常用扩展" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-brands:chrome" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->常用扩展<!----></a></li><li class="vp-dropdown-item"><h4 class="vp-dropdown-subtitle">Intership</h4><ul class="vp-dropdown-subitems"><li class="vp-dropdown-subitem"><a class="route-link auto-link" href="/jasonNote/apps/topic/" aria-label="搜广推" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:dice-d20" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->搜广推<!----></a></li></ul></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="生活"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bed-pulse" height="1em" sizing="height"></iconify-icon>生活<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/family/Diet.html" aria-label="健康饮食" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:wheat-awn-circle-exclamation" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->健康饮食<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/jasonNote/family/Coupon.html" aria-label="网购攻略" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:cart-shopping" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->网购攻略<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="工具"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:toolbox" height="1em" sizing="height"></iconify-icon>工具<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://www.aishort.top/" aria-label="ChatGPT SC" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bolt" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->ChatGPT SC<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://prompt.newzone.top/" aria-label="IMGPrompt" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:image" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->IMGPrompt<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://tools.newzone.top/json-translate" aria-label="多语言翻译" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:language" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->多语言翻译<!----></a></li><li class="vp-dropdown-item"><a class="auto-link external-link" href="https://nav.newzone.top/" aria-label="工具收藏" rel="noopener noreferrer" target="_blank" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:bars" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->工具收藏<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://discord.gg/PZTQfJ4GjX" target="_blank" rel="noopener noreferrer" aria-label="discord"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 640" style="width:1.25rem;height:1.25rem;vertical-align:middle"><path d='m534.42,126.58a1.57,1.57 0 0 0 -0.79,-0.73a507.33,507.33 0 0 0 -125.19,-38.81a1.9,1.9 0 0 0 -2.01,0.95a352.96,352.96 0 0 0 -15.58,32.01a468.41,468.41 0 0 0 -140.6,0a323.75,323.75 0 0 0 -15.82,-32.01a1.98,1.98 0 0 0 -2.02,-0.95a505.9,505.9 0 0 0 -125.19,38.81a1.79,1.79 0 0 0 -0.82,0.7c-79.73,119.07 -101.57,235.21 -90.86,349.9a2.11,2.11 0 0 0 0.81,1.44a510.06,510.06 0 0 0 153.56,77.6a1.99,1.99 0 0 0 2.15,-0.71a364.19,364.19 0 0 0 31.42,-51.08a1.95,1.95 0 0 0 -1.07,-2.71a335.92,335.92 0 0 1 -47.98,-22.85a1.98,1.98 0 0 1 -0.19,-3.27c3.22,-2.42 6.44,-4.93 9.53,-7.46a1.9,1.9 0 0 1 1.99,-0.27c100.65,45.94 209.61,45.94 309.07,0a1.89,1.89 0 0 1 2.01,0.24c3.09,2.54 6.31,5.07 9.55,7.49a1.97,1.97 0 0 1 -0.17,3.27a315.25,315.25 0 0 1 -48,22.83a1.97,1.97 0 0 0 -1.05,2.73a409.02,409.02 0 0 0 31.4,51.05a1.95,1.95 0 0 0 2.15,0.73a508.37,508.37 0 0 0 153.81,-77.59a1.97,1.97 0 0 0 0.81,-1.41c12.82,-132.61 -21.48,-247.79 -90.93,-349.9zm-315.91,280.03c-30.3,0 -55.27,-27.81 -55.27,-61.96s24.48,-61.96 55.27,-61.96c31.03,0 55.76,28.05 55.27,61.96c0,34.15 -24.48,61.96 -55.27,61.96zm204.35,0c-30.3,0 -55.27,-27.81 -55.27,-61.96s24.48,-61.96 55.27,-61.96c31.03,0 55.76,28.05 55.27,61.96c0,34.15 -24.23,61.96 -55.27,61.96z' fill="currentColor"/></svg></a></div><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/jasonaowu/jasonNote" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><button type="button" class="slimsearch-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="slimsearch-placeholder">搜索</div><div class="slimsearch-key-hints"><kbd class="slimsearch-key">Ctrl</kbd><kbd class="slimsearch-key">K</kbd></div></button><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/BERT.html" aria-label="BERT" iconsizing="both"><!---->BERT<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/DeepSeek%E7%B3%BB%E5%88%97.html" aria-label="DeepSeek系列" iconsizing="both"><!---->DeepSeek系列<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/GPT.html" aria-label="GPT系列" iconsizing="both"><!---->GPT系列<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/Qwen.html" aria-label="Qwen" iconsizing="both"><!---->Qwen<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->DeepSeek系列</h1><div class="page-info"><!----><!----><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon" name="word"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 1540 字</span><meta property="wordCount" content="1540"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 5 分钟</span><meta property="timeRequired" content="PT5M"></span><!----></div><hr></div><!----><!----><div class="theme-hope-content" vp-content><h1 id="问题和结论" tabindex="-1"><a class="header-anchor" href="#问题和结论"><span>问题和结论</span></a></h1><ol><li>MOE 能否做知识蒸馏？ <span style="color:blue;background-color:lightblue;"><strong>可以</strong></span></li><li>MOE 相比 Dense 的优势？ <span style="color:blue;background-color:lightblue;"><strong>节约计算成本</strong></span></li><li>什么结构能用 MOE？ <span style="color:blue;background-color:lightblue;"><strong>任何 FFN → 多任务问题</strong></span></li><li>在已知 MOE 有负载不均衡问题的前提下，为啥目前大模型都开始抛弃传统 Transformer 架构，转投 MOE？ <span style="color:blue;background-color:lightblue;"><strong>便宜</strong></span></li><li>个人原来的理解：MOE 只能节约训练和推理的计算量，不能节约存储量；模型蒸馏可以节约计算量，也可以节约存储量，是否正确？ <span style="color:blue;background-color:lightblue;"><strong>正确</strong></span></li></ol><blockquote><p><strong>错误观点</strong>：</p><ol><li>MOE 是为了减小网络结构？ ❌，相反，MOE的初衷是为了在保证较低计算量的同时，增加模型参数，使模型更强</li></ol><p>原来以为MOE是针对深层网络做的优化，将深层网络变为浅层网络，但是实际是将中间层参数数量从 N，降低为 N/E，分散到E个专家上</p></blockquote><h2 id="moe-模型基本特性" tabindex="-1"><a class="header-anchor" href="#moe-模型基本特性"><span>MoE 模型基本特性</span></a></h2><p>与稠密模型相比，对于给定的计算预算，MoE 模型提供更高效的训练。这是因为门控网络仅将 token 发送到一部分专家，从而减少了计算负载。因此，模型的容量（其参数总数）可以增加，而不会成比例地增加计算需求。在推理期间，仅使用部分专家，因此 MoE 能够执行比稠密模型更快的推理。但是，整个模型需要加载到内存中，而仅仅是正在使用的专家。</p><p>MoE 中实现更高计算效率的稀疏性来自于这样一个事实：特定的 token 只会被路由到一部分专家。专家的数量以及如何选择专家取决于门控网络的实现，但一种常见的方法是 top k。门控网络首先预测每个专家的概率值，然后将 token 路由到 top k 个专家以获得输出。但是，如果所有 token 始终都发送到相同的专家子集，则训练效率会降低，而其他专家最终会训练不足。为了缓解这个问题，引入了负载均衡损失，以鼓励均匀路由到所有专家。</p><p>专家的数量和选择 top k 个专家是设计 MoE 的重要因素。更多的专家数量允许扩展到更大的模型，而不会增加计算成本。这意味着模型具有更高的学习能力，但是，超过某个点后，性能增益往往会减少。选择的专家数量需要与服务模型的推理成本相平衡，因为整个模型都需要加载到内存中。同样，在选择 top k 时，训练期间较低的 top k 会导致较小的矩阵乘法，如果通信成本足够大，则会浪费计算资源。但是，在推理期间，较高的 top k 通常会导致较慢的推理速度。</p><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ul><li><a href="https://kevincheung2259.github.io/2024/09/13/MOE-Intro/index.html" target="_blank" rel="noopener noreferrer">MOE 介绍</a></li><li><a href="https://deepseek.csdn.net/67fa2941da5d787fd5cb6acb.html" target="_blank" rel="noopener noreferrer">DeepSeek 技术解析</a></li></ul><h2 id="研究问题" tabindex="-1"><a class="header-anchor" href="#研究问题"><span>研究问题</span></a></h2><p>以 Transformer 原文中 FFN 部分的参数量进行计算，如果换成 MOE 架构，参数量是多少，为啥能够节约计算时间？</p><h2 id="deepseek-研究脉络" tabindex="-1"><a class="header-anchor" href="#deepseek-研究脉络"><span>DeepSeek 研究脉络</span></a></h2><p>💡 <strong>从稠密模型到混合专家，再到推理方向</strong></p><p>回顾 DeepSeek 过去一年多发表的核心论文，我们大致能将其研究分为两条主要脉络：</p><ul><li><strong>基座模型（Foundation Models）</strong>：从最早的 Dense（稠密）结构一路演进到 MOE（混合专家）模式，并在这个过程中不断发明和采用新的高效训练算法。</li><li><strong>推理能力（Reasoning）</strong>：包括解数学题、代码生成、逻辑问答乃至定理证明等，更强调大模型的&quot;思考深度&quot;，并在如何进行强化学习方面进行了连续多次创新。</li></ul><p>在阅读这份逐篇解读之前，可以先记住 DeepSeek 的几大特色：对实验和数据极度重视、有足够的冒险精神尝试新架构和新算法、且真正愿意分享内部研究细节，为社区提供可复现的技术报告。</p><h1 id="moe-基本原理" tabindex="-1"><a class="header-anchor" href="#moe-基本原理"><span>MOE 基本原理</span></a></h1><p>MOE 全称是 Mixture of Experts，也就是混合专家模型</p><h2 id="最最最原始版" tabindex="-1"><a class="header-anchor" href="#最最最原始版"><span>最最最原始版</span></a></h2><h3 id="组成" tabindex="-1"><a class="header-anchor" href="#组成"><span>组成</span></a></h3><ol><li><strong>稀疏 MOE 层</strong>：n 个专家 FFN</li><li><strong>路由</strong>：token 到 top-K 个专家</li></ol><p><img src="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=NWVjNjk1ZmU5YmUyNDA3ZGZlZGQ3MzljZWU5NzVhZWJfQ1pESDhYRFRIM1QwNVEwTzRJSVNCdWZOS1FkNHJ5Q2JfVG9rZW46UnA1M2Jja25Cb2c2MlJ4TTc5SGNtMFE1bmdlXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA" alt="MOE 基本结构" loading="lazy"><img src="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=NzMzNzMyZWE5ZDFkYjc1MTUwODE0OTViZjI4MWFjNGNfdTRrMVc0UXJnemtRekFWNTNhYzZwMXRBc2plak9naWZfVG9rZW46WE05UmJ3b0VNb05OUTN4MjdVTGNsSXYxbkNuXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA" alt="MOE 结构图2" loading="lazy"></p><h3 id="ffn-对比" tabindex="-1"><a class="header-anchor" href="#ffn-对比"><span>FFN 对比</span></a></h3><ul><li><strong>Vs Transformer</strong></li></ul><figure><img src="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=YjMwMmY0OTI2ZDQ0M2U5MDE2NzlkODc0OTQzNGFiMTNfY1Q3VlhRTGN3VjE4R05QVmF5UEoxbXpVR3hoeHRsb0dfVG9rZW46QU9ieWJvUjY1b3F4SU14c09Cb2NYbEd2bnJjXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA" alt="Transformer 对比" tabindex="0" loading="lazy"><figcaption>Transformer 对比</figcaption></figure><ul><li><strong>一般的 gating network 的计算，便于和 deepseek 做对比</strong></li></ul><figure><img src="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=NDkyY2U0NDQ0MzUyYjlmYTg0M2RiMzI2ZTBiNzFiMWZfc1ZwOFpyS1NhdUUwNnJFbGZnUGZkZDRaZnYzb2VpelRfVG9rZW46VEdOQWJnTURab1V1alZ4b3BHQmNmSm5mbldiXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA" alt="Gating Network" tabindex="0" loading="lazy"><figcaption>Gating Network</figcaption></figure><h3 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势</span></a></h3><ul><li>相比 dense 模型，<strong>预训练速度更快</strong></li><li>相比同参数量模型，<strong>推理速度更快</strong></li><li>但是需要高 VRAM，因为所有专家都加载在内存中</li></ul><h2 id="switch-transformer" tabindex="-1"><a class="header-anchor" href="#switch-transformer"><span>Switch Transformer</span></a></h2><figure><img src="https://s08a4grxpw8.feishu.cn/space/api/box/stream/download/asynccode/?code=YmE1MzdjZjA0Y2M2YmJhNzZjNGI3NTEyNzE2NmRlZTBfd1FBOXByWnVhUmt1Tm1xanRTcm5ZU0ZKRnhRMTlKTVhfVG9rZW46Umw1dWJJdFdpb3JqcU14NlZsNWNKa2pSbmhkXzE3NDU1ODU4NTk6MTc0NTU4OTQ1OV9WNA" alt="Switch Transformer" tabindex="0" loading="lazy"><figcaption>Switch Transformer</figcaption></figure><h1 id="deepseek-moe-2024-01" tabindex="-1"><a class="header-anchor" href="#deepseek-moe-2024-01"><span>DeepSeek MOE(2024.01)</span></a></h1><p>DeepSeek-V1 应该是 2023 年 12 月的 DeepSeek LLM Base 和 Chat 模型，是稠密模型。</p><p>DeepSeek-V2 及其之后的模型用的都是 MoE 了。</p><p><a href="https://arxiv.org/pdf/2401.06066" target="_blank" rel="noopener noreferrer">DeepSeek MOE 原文</a></p><h2 id="背景" tabindex="-1"><a class="header-anchor" href="#背景"><span>背景</span></a></h2><ul><li>LLM 中，扩展模型参数时节约成本，故使用 MoE</li><li>Deepseek MOE 就是为了通过更加高效的机制来确保专家之间的任务分配具有更高的专门化性</li><li>无法确保专家的专门化。这种重叠会导致专家没有获得足够的独特知识，也使得专家之间的差异化不明显，限制了模型的性能和效率 <ul><li><strong>知识混杂性（Knowledge Hybridity）</strong>：在传统的 MoE 架构中，通常只使用有限数量的专家（例如 8 个或 16 个）。当某个 token 被分配给某个专家时，这些专家所涵盖的知识往往是多样化的，因此该专家的参数会试图同时存储和处理非常不同类型的知识。这种知识的多样性和复杂性导致专家的知识无法高度专注和聚焦，从而难以在同一模型中有效地利用这些不同类型的知识。</li></ul></li></ul></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/4/25 14:51:15</span></div><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/BERT.html" aria-label="BERT" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->BERT</div></a><a class="route-link auto-link next" href="/jasonNote/LLM/%E7%83%AD%E9%97%A8%E6%A8%A1%E5%9E%8B/GPT.html" aria-label="GPT系列" iconsizing="both"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">GPT系列<!----></div></a></nav><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;"><div style="display: flex;
align-items: center;
justify-content: center;
height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);
--icon-size: 48px;
display: inline-block;
width: var(--icon-size);
height: var(--icon-size);
background-color: currentcolor;
-webkit-mask-image: var(--loading-icon);
mask-image: var(--loading-icon);
"></span></div></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><!----><div class="vp-copyright">
  版权声明：自由转载 - 非商用 - 非衍生 - 保持署名<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" target="_blank" rel="noopener noreferrer">（创意共享 4.0 许可证）</a>|
  Copyright © 2023-present LearnData</a>
  </div></footer></div><!--]--><!--[--><!----><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/jasonNote/assets/js/runtime~app.7e62fd9b.js" defer></script><script src="/jasonNote/assets/js/1316.1b8cebb6.js" defer></script><script src="/jasonNote/assets/js/app.a956ffe5.js" defer></script>
    <!-- 看板娘区块 -->
    <script src="/jasonNote/live2d-widget/autoload.js"></script>
    <!-- End 看板娘区块 -->
  </body>
</html>
